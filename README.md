# Deep Learning with Label Noise / Noisy Labels

This repo consists of collection of papers and repos on the topic of deep learning by noisy labels. All methods listed below are briefly explained in the paper [Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey](https://arxiv.org/abs/1912.05170). More information about the topic can also be found on the survey.

|Year|Type|Conf|Repo|Title|
|----|----|----|----|-----|
|2021|ML||[Pt](https://github.com/gorkemalgan/MetaLabelNet)|[MetaLabelNet: Learning to Generate Soft-Labels from Noisy-Labels](https://arxiv.org/abs/2103.10869)|
|2020|ML|ICPR|[Pt](https://github.com/gorkemalgan/MSLG_noisy_label)|[Meta Soft Label Generation for Noisy Labels](https://arxiv.org/abs/2007.05836)|
|2020|RL|||[Learning Adaptive Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2002.06482)|
|2020|LNC|||[ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks](https://arxiv.org/abs/2005.03788)|
|2020|DP|||[Identifying Mislabeled Data using the Area Under the Margin Ranking](https://arxiv.org/abs/2001.10528)|
|2020|R|||[Limited Gradient Descent: Learning With Noisy Labels](https://arxiv.org/abs/1811.08117)|
|2020|NC|||[Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning](http://arxiv.org/abs/2006.07805)|
|2020|LNC|||[Temporal Calibrated Regularization for Robust Noisy Label Learning](https://arxiv.org/abs/2007.00240)|
|2020|NC|||[Parts-dependent Label Noise: Towards Instance-dependent Label Noise](http://arxiv.org/abs/2006.07836)|
|2020|NC|||[Class2Simi: A New Perspective on Learning with Label Noise](https://arxiv.org/abs/2006.07831)|
|2020|LNC|||[Learning from Noisy Labels with Noise Modeling Network](https://arxiv.org/abs/2005.00596)|
|2020|LNC|||[ExpertNet: Adversarial Learning and Recovery Against Noisy Labels](https://arxiv.org/abs/2007.05305)|
|2020|R||[Pt](https://github.com/shengliu66/ELR)|[Early-Learning Regularization Prevents Memorization of Noisy Labels](https://arxiv.org/pdf/2007.00151.pdf)|
|2020|LNC|||[ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks](https://arxiv.org/abs/2005.03788)|
|2020|SC|CVPR|[Pt](https://github.com/hongxin001/JoCoR)|[Combating Noisy Labels by Agreement: A Joint Training Method with Co-Regularization](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wei_Combating_Noisy_Labels_by_Agreement_A_Joint_Training_Method_with_CVPR_2020_paper.pdf)|
|2020|SIW|CVPR|[Tf](https://github.com/google-research/google-research/tree/master/ieg)|[Distilling Effective Supervision from Severe Label Noise](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Distilling_Effective_Supervision_From_Severe_Label_Noise_CVPR_2020_paper.pdf)|
|2020|NC|CVPR| |[Training Noise-Robust Deep Neural Networks via Meta-Learning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Training_Noise-Robust_Deep_Neural_Networks_via_Meta-Learning_CVPR_2020_paper.pdf)|
|2020|LNC|CVPR||[Global-Local GCN: Large-Scale Label Noise Cleansing for Face Recognition](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Global-Local_GCN_Large-Scale_Label_Noise_Cleansing_for_Face_Recognition_CVPR_2020_paper.html)|
|2020|SIW|ECCV||[Graph convolutional networks for learning with few clean and many noisy labels](https://arxiv.org/abs/1910.00324)|
|2020|SIW|ECCV||[NoiseRank: Unsupervised Label Noise Reduction with Dependence Models](https://arxiv.org/abs/2003.06729)|
|2020|R|ICLR||[Simple and Effective Regularization Methods for Training on Noisily Labeled Data with Generalization Guarantee](https://arxiv.org/abs/1905.11368)|
|2020|R|ICLR||[Can Gradient Clipping Mitigate Label Noise?](https://openreview.net/pdf?id=rklB76EKPr)|
|2020|SSL|ICLR|[Pt](https://github.com/LiJunnan1992/DivideMix)|[DivideMix: Learning with Noisy Labels as Semi-supervised Learning](https://arxiv.org/abs/2002.07394)|
|2020|SC|AAAI||[Self-Paced Robust Learning for Leveraging Clean Labels in Noisy Data](http://people.cs.vt.edu/~ctlu/Publication/2020/AAAI-ZhangX-8567-Proceedings.pdf)|
|2020|LNC|IJCAI||[Learning with Noise: Improving Distantly-Supervised Fine-grained Entity Typing via Automatic Relabeling](https://www.ijcai.org/Proceedings/2020/0527.pdf)|
|2020|SIW|IJCAI||[Label Distribution for Learning with Noisy Labels](https://www.ijcai.org/Proceedings/2020/356)|
|2020|RL|IJCAI||[Can Cross Entropy Loss Be Robust to Label Noise?](https://www.ijcai.org/Proceedings/2020/305)|
|2020|SC|WACV||[Learning from noisy labels via discrepant collaborative training](https://openaccess.thecvf.com/content_WACV_2020/html/Han_Learning_from_Noisy_Labels_via_Discrepant_Collaborative_Training_WACV_2020_paper.html)|
|2020|LNC|WACV||[A novel self-supervised re-labeling approach for training with noisy labels](https://openaccess.thecvf.com/content_WACV_2020/html/Mandal_A_Novel_Self-Supervised_Re-labeling_Approach_for_Training_with_Noisy_Labels_WACV_2020_paper.html)|
|2020|SC|ICML||[Searching to Exploit Memorization Effect in Learning from Corrupted Labels](https://arxiv.org/abs/1911.02377)|
|2020|ML|ICML||[SIGUA: Forgetting May Make Learning with Noisy Labels More Robust](https://proceedings.icml.cc/static/paper_files/icml/2020/705-Paper.pdf)|
|2020|R|ICML|[Pt](https://github.com/hrayrhar/limit-label-memorization)|[Improving Generalization by Controlling Label-Noise Information in Neural Network Weights](https://arxiv.org/abs/2002.07933)|
|2020|RL|ICML||[Normalized Loss Functions for Deep Learning with Noisy Labels](https://proceedings.icml.cc/static/paper_files/icml/2020/1502-Paper.pdf)|
|2020|RL|ICML||[Peer Loss Functions: Learning from Noisy Labels without Knowing Noise Rates](https://arxiv.org/abs/1910.03231)|
|2020|SC|ICML||[Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels](https://arxiv.org/abs/1911.09781)|
|2020|O|ICML||[Deep k-NN for Noisy Labels](https://arxiv.org/abs/2004.12289)|
|2020|LNC|ICML||[Error-Bounded Correction of Noisy Labels](https://proceedings.icml.cc/static/paper_files/icml/2020/2506-Paper.pdf)|
|2020|O|ICML||[Does label smoothing mitigate label noise?](https://arxiv.org/abs/2003.02819)|
|2020|DP|ICML||[Learning with Bounded Instance- and Label-dependent Label Noise](https://arxiv.org/abs/1709.03768)|
|2020|O|ICML||[Training Binary Neural Networks through Learning with Noisy Supervision](https://proceedings.icml.cc/static/paper_files/icml/2020/181-Paper.pdf)|
|2019|SIW|NIPS|[Pt](https://github.com/xjtushujun/meta-weight-net)|[Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting](https://arxiv.org/abs/1902.07379)|
|2019|RL |ICML||[On Symmetric Losses for Learning from Corrupted Labels](https://arxiv.org/abs/1901.09314)|
|2019|O  |ICLR|[Pt](https://github.com/orlitany/SOSELETO)|[SOSELETO: A Unified Approach to Transfer Learning and Training with Noisy Labels](https://arxiv.org/abs/1805.09622)|
|2019|LNC|ICLR||[An Energy-Based Framework for Arbitrary Label Noise Correction](https://openreview.net/forum?id=Hyxu6oAqYX)|
|2019|NC |NIPS|[Pt](https://github.com/xiaoboxia/T-Revision)|[Are Anchor Points Really Indispensable in Label-Noise Learning?](https://arxiv.org/abs/1906.00189)|
|2019|O  |NIPS|[Pt](https://github.com/snow12345/Combinatorial_Classification)|[Combinatorial Inference against Label Noise](https://papers.nips.cc/paper/8401-combinatorial-inference-against-label-noise)|
|2019|RL |NIPS|[Pt](https://github.com/Newbeeer/L_DMI)|[L_DMI : A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise](https://arxiv.org/abs/1909.03388)|
|2019|O  |CVPR||[MetaCleaner: Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition](http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_MetaCleaner_Learning_to_Hallucinate_Clean_Representations_for_Noisy-Labeled_Visual_Recognition_CVPR_2019_paper.html)|
|2019|LNC|ICCV||[O2U-Net: A Simple Noisy Label Detection Approach for Deep Neural Networks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_O2U-Net_A_Simple_Noisy_Label_Detection_Approach_for_Deep_Neural_ICCV_2019_paper.pdf)|
|2019|SC |ICCV|[*](http://www.cbsr.ia.ac.cn/users/xiaobowang/)|[Co-Mining: Deep Face Recognition with Noisy Labels](http://openaccess.thecvf.com/content_ICCV_2019/papers/Wang_Co-Mining_Deep_Face_Recognition_With_Noisy_Labels_ICCV_2019_paper.pdf)|
|2019|O  |     ||[NLNL: Negative Learning for Noisy Labels](https://arxiv.org/abs/1908.07387)
|2019|R  |     |[Pt](https://github.com/hendrycks/pre-training)|[Using Pre-Training Can Improve Model Robustness and Uncertainty](https://arxiv.org/abs/1901.09960)
|2019|SSL|     ||[Robust Learning Under Label Noise With Iterative Noise-Filtering](https://arxiv.org/abs/1906.00216)
|2019|ML |CVPR |[Pt](https://github.com/LiJunnan1992/MLNT)|[Learning to Learn from Noisy Labeled Data](https://arxiv.org/abs/1812.05214)
|2019|ML |     ||[Pumpout: A Meta Approach for Robustly Training Deep Neural Networks with Noisy Labels](https://arxiv.org/abs/1809.11008)
|2019|RL |     |[Keras](https://github.com/YisenWang/symmetric_cross_entropy_for_noisy_labels)|[Symmetric Cross Entropy for Robust Learning with Noisy Labels](https://arxiv.org/abs/1908.06112)
|2019|RL |     |[Caffe](https://github.com/XinshaoAmosWang/Improving-Mean-Absolute-Error-against-CCE)|[Improved Mean Absolute Error for Learning Meaningful Patterns from Abnormal Training Data](https://arxiv.org/abs/1903.12141)
|2019|LQA|CVPR ||[Learning From Noisy Labels By Regularized Estimation Of Annotator Confusion](https://arxiv.org/abs/1902.03680)
|2019|SIW|CVPR |[Caffe](https://github.com/huangyangyu/NoiseFace)|[Noise-Tolerant Paradigm for Training Face Recognition CNNs](https://arxiv.org/abs/1903.10357)
|2019|SIW|ICML |[Pt](https://github.com/thulas/dac-label-noise)|[Combating Label Noise in Deep Learning Using Abstention](https://arxiv.org/abs/1905.10964)
|2019|SIW|     ||[Robust Learning at Noisy Labeled Medical Images: Applied to Skin Lesion Classification](https://arxiv.org/abs/1901.07759)
|2019|SC |ICML |[Keras](https://github.com/chenpf1025/noisy_label_understanding_utilizing)|[Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels](https://arxiv.org/abs/1905.05040)
|2019|SC |ICML |[Pt](https://github.com/bhanML/coteaching_plus)|[How does Disagreement Help Generalization against Label Corruption?](https://arxiv.org/abs/1901.04215)
|2019|SC |CVPR ||[Learning a Deep ConvNet for Multi-label Classification with Partial Labels](https://arxiv.org/abs/1902.09720)
|2019|SC |     ||[Curriculum Loss: Robust Learning and Generalization against Label Corruption](https://arxiv.org/abs/1905.10045)
|2019|SC |     ||[SELF: Learning to Filter Noisy Labels with Self-Ensembling](https://arxiv.org/abs/1910.01842)
|2019|LNC |CVPR |[Pt](https://github.com/jx-zhong-for-academic-purpose/GCN-Anomaly-Detection)|[Graph Convolutional Label Noise Cleaner: Train a Plug-and-play Action Classifier for Anomaly Detection](https://arxiv.org/abs/1903.07256)
|2019|LNC|ICCV ||[Photometric Transformer Networks and Label Adjustment for Breast Density Prediction](https://arxiv.org/abs/1905.02906)
|2019|LNC|CVPR |[Pt](https://github.com/yikun2019/PENCIL)|[Probabilistic End-to-end Noise Correction for Learning with Noisy Labels](https://arxiv.org/abs/1903.07788)
|2019|LNC|TGRS |[Matlab](https://github.com/junjun-jiang/RLPA)|[Hyperspectral image classification in the presence of noisy labels](https://arxiv.org/abs/1809.04212)
|2019|LNC|ICCV ||[Deep Self-Learning From Noisy Labels](https://arxiv.org/abs/1908.02160)
|2019|NC |AAAI |[Tf](https://github.com/Sunarker/Safeguarded-Dynamic-Label-Regression-for-Noisy-Supervision)|[Safeguarded Dynamic Label Regression for Noisy Supervision](https://arxiv.org/abs/1903.02152)
|2019|NC |ICML |[Pt](https://github.com/PaulAlbert31/LabelNoiseCorrection)|[Unsupervised Label Noise Modeling and Loss Correction](https://arxiv.org/abs/1904.11238)
|2018|O  |ECCV ||[Learning with Biased Complementary Labels](https://arxiv.org/abs/1711.09535)
|2018|O  |     ||[Robust Determinantal Generative Classifier for Noisy Labels and Adversarial Attacks](https://openreview.net/forum?id=rkle3i09K7)
|2018|R  |ICLR |[Keras](https://github.com/xingjunm/dimensionality-driven-learning)|[Dimensionality Driven Learning for Noisy Labels](https://arxiv.org/abs/1806.02612)
|2018|R  |ECCV ||[Deep bilevel learning](https://arxiv.org/abs/1809.01465)
|2018|SSL|WACV ||[A semi-supervised two-stage approach to learning from noisy labels](https://arxiv.org/abs/1802.02679)
|2018|ML |     ||[Improving Multi-Person Pose Estimation using Label Correction](https://arxiv.org/abs/1811.03331)
|2018|RL |NIPS ||[Generalized cross entropy loss for training deep neural networks with noisy labels](https://arxiv.org/abs/1805.07836)
|2018|LQA|ICLR |[Repo](https://github.com/khetan2/MBEM)|[Learning From Noisy Singly-Labeled Data](https://arxiv.org/abs/1712.04577)
|2018|LQA|AAAI ||[Deep learning from crowds](https://arxiv.org/abs/1709.01779)
|2018|SIW|CVPR |[Repo](https://github.com/YisenWang/Iterative_learning)|[Iterative Learning With Open-Set Noisy Labels](https://arxiv.org/abs/1804.00092)               
|2018|SIW|     |[Tf](https://github.com/uber-research/learning-to-reweight-examples)|[Learning to Reweight Examples for Robust Deep Learning](https://arxiv.org/abs/1803.09050)
|2018|SIW|CVPR |[Tf](https://github.com/kuanghuei/clean-net)|[Cleannet: Transfer Learning for Scalable Image Classifier Training with Label Noise](https://arxiv.org/abs/1711.07131)
|2018|SIW|     ||[ChoiceNet: Robust Learning by Revealing Output Correlations](https://arxiv.org/abs/1805.06431)
|2018|SIW|IEEE ||[Multiclass Learning with Partially Corrupted Labels](https://ieeexplore.ieee.org/abstract/document/7929355/)
|2018|SC |NIPS |[Pt](https://github.com/bhanML/Co-teaching)|[Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels](https://arxiv.org/abs/1804.06872)
|2018|SC |IEEE ||[Progressive Stochastic Learning for Noisy Labels](https://ieeexplore.ieee.org/document/8281022)
|2018|SC |ECCV |[Sklearn](https://github.com/MalongTech/research-curriculumnet)|[Curriculumnet: Weakly supervised learning from large-scale web images](https://arxiv.org/abs/1808.01097)
|2018|LNC|CVPR | [Chainer](https://github.com/DaikiTanaka-UT/JointOptimization)|[Joint Optimization Framework for Learning with Noisy Labels](https://arxiv.org/abs/1803.11364)
|2018|LNC|TIFS | [Pt](https://github.com/AlfredXiangWu/LightCNN), [Caffe](https://github.com/AlfredXiangWu/face_verification_experiment), [Tf](https://github.com/yxu0611/Tensorflow-implementation-of-LCNN)|[A light CNN for deep face representation with noisy labels](https://arxiv.org/abs/1511.02683)
|2018|LNC|WACV ||[Iterative cross learning on noisy labels](https://ieeexplore.ieee.org/document/8354192)
|2018|NC |NIPS |[Pt](https://github.com/mmazeika/glc)|[Using trusted data to train deep networks on labels corrupted by severe noise](https://arxiv.org/abs/1802.05300)
|2018|NC |ISBI ||[Training a neural network based on unreliable human annotation of medical images](https://ieeexplore.ieee.org/document/8363518/)
|2018|NC |IEEE ||[Deep learning from noisy image labels with quality embedding](https://arxiv.org/abs/1711.00583)
|2018|NC |NIPS |[Tf](https://github.com/bhanML/Masking) | [Masking: A new perspective of noisy supervision](https://arxiv.org/abs/1805.08193)
|2017|O  |     ||[Learning with Auxiliary Less-Noisy Labels](https://ieeexplore.ieee.org/document/7448430)
|2017|R  |     ||[Regularizing neural networks by penalizing confident output distributions](https://arxiv.org/abs/1701.06548)
|2017|R  |     |[Pt](https://github.com/facebookresearch/mixup-cifar10)|[mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)
|2017|MIL|CVPR ||[Attend in groups: a weakly-supervised deep learning framework for learning from web data](https://arxiv.org/abs/1611.09960)
|2017|ML |ICCV ||[Learning from Noisy Labels with Distillation](https://arxiv.org/abs/1703.02391)
|2017|ML |     ||[Avoiding your teacher's mistakes: Training neural networks with controlled weak supervision](https://arxiv.org/abs/1711.00313)
|2017|ML |     ||[Learning to Learn from Weak Supervision by Full Supervision](https://arxiv.org/abs/1711.11383)
|2017|RL |AAAI ||[Robust Loss Functions under Label Noise for Deep Neural](https://arxiv.org/abs/1712.09482)
|2017|LQA|ICLR ||[Who Said What: Modeling Individual Labelers Improves Classification](https://arxiv.org/abs/1703.08774)
|2017|LQA|CVPR ||[Lean crowdsourcing: Combining humans and machines in an online system](http://ieeexplore.ieee.org/document/8100130/)
|2017|SC |NIPS |[Tf](https://github.com/emalach/UpdateByDisagreement)|[Decoupling" when to update" from" how to update"](https://arxiv.org/abs/1706.02613)
|2017|SC |NIPS |[Tf*](https://github.com/songhwanjun/ActiveBias)|[Active bias: Training more accurate neural networks by emphasizing high variance samples](https://arxiv.org/abs/1704.07433)
|2017|SC |     |[Tf](https://github.com/google/mentornet)|[MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels](https://arxiv.org/abs/1712.05055)
|2017|SC |     |[Sklearn](https://github.com/cgnorthcutt/rankpruning)|[Learning with confident examples: Rank pruning for robust classification with noisy labels](https://arxiv.org/abs/1705.01936)
|2017|SC |NIPS ||[Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks](https://arxiv.org/abs/1706.00038)
|2017|LNC|IEEE ||[Self-Error-Correcting Convolutional Neural Network for Learning with Noisy Labels](http://ieeexplore.ieee.org/document/7961730/)
|2017|LNC|IEEE ||[Improving crowdsourced label quality using noise correction](https://ieeexplore.ieee.org/document/7885126/)
|2017|LNC|     ||[Fidelity-weighted learning](https://arxiv.org/abs/1711.02799)
|2017|LNC|CVPR ||[Learning From Noisy Large-Scale Datasets With Minimal Supervision](https://arxiv.org/abs/1701.01619)
|2017|NC |CVPR |[Keras](https://github.com/giorgiop/loss-correction)|[Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach](https://arxiv.org/abs/1609.03683)
|2017|NC |ICLR |[Keras](https://github.com/udibr/noisy_labels)|[Training Deep Neural-Networks Using a Noise Adaptation Layer](https://openreview.net/forum?id=H12GRgcxg)
|2016|EM |KBS  ||[A robust multi-class AdaBoost algorithm for mislabeled noisy data](https://dl.acm.org/citation.cfm?id=2932672)
|2016|R  |CVPR ||[Rethinking the inception architecture for computer vision](https://arxiv.org/abs/1512.00567)
|2016|SSL|AAAI ||[Robust semi-supervised learning through label aggregation](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12312)
|2016|ML |NC   ||[Noise detection in the Meta-Learning Level](https://www.sciencedirect.com/science/article/abs/pii/S0925231215005482)
|2016|RL |     ||[On the convergence of a family of robust losses for stochastic gradient descent](https://arxiv.org/abs/1605.01623)
|2016|RL |ICML ||[Loss factorization, weakly supervised learning and label noise robustness](https://arxiv.org/abs/1602.02450)
|2016|SIW|ICLR |[Matlab](https://github.com/azadis/AIR)|[Auxiliary image regularization for deep cnns with noisy labels](https://arxiv.org/abs/1511.07069)
|2016|SIW|CVPR |[Caffe](https://github.com/imisra/latent-noise-icnm)|[Seeing Through the Human Reporting Bias: Visual Classifiers From Noisy Human-Centric Labels](https://arxiv.org/abs/1512.06974)
|2016|SC |ECCV |[Repo](https://github.com/google/goldfinch)|[The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition](https://arxiv.org/abs/1511.06789)
|2016|NC |ICDM |[Matlab](https://github.com/ijindal/Noisy_Dropout_regularization)|[Learning deep networks from noisy labels with dropout regularization](https://arxiv.org/abs/1705.03419)
|2016|NC |CASSP|[Keras](https://github.com/udibr/noisy_labels) |[Training deep neural-networks based on unreliable labels](https://ieeexplore.ieee.org/document/7472164)
|2015|O  |     ||[Learning discriminative reconstructions for unsupervised outlier removal](https://ieeexplore.ieee.org/document/7410534)
|2015|EM |     ||[Rboost: label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners](http://ieeexplore.ieee.org/document/7273923/)
|2015|MIL|CVPR ||[Visual recognition by learning from web data: A weakly supervised domain generalization approach](http://ieeexplore.ieee.org/document/7298894/)
|2015|RL |NIPS ||[Learning with symmetric label noise: The importance of being unhinge](https://arxiv.org/abs/1505.07634)
|2015|RL |NC   ||[Making risk minimization tolerant to label noise](https://arxiv.org/abs/1403.3610)
|2015|LQA|     ||[Deep classifiers from image tags in the wild](https://dl.acm.org/citation.cfm?id=2814821)
|2015|SIW|TPAMI|[Pt](https://github.com/xiaoboxia/Classification-with-noisy-labels-by-importance-reweighting)|[Classification with noisy labels by importance reweighting](https://arxiv.org/pdf/1411.7718)
|2015|SC |ICCV |[Website](http://xinleic.xyz/web.html)|[Webly supervised learning of convolutional networks](https://arxiv.org/abs/1505.01554)
|2015|NC |CVPR |[Caffe](https://github.com/Cysu/noisy_label) | [Learning From Massive Noisy Labeled Data for Image Classification](https://ieeexplore.ieee.org/document/7298885)
|2015|NC |ICLR ||[Training Convolutional Networks with Noisy Labels](https://arxiv.org/abs/1406.2080)
|2014|R  |     ||[Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572)
|2014|R  |JMLR ||[Dropout: a simple way to prevent neural networks from overfitting](http://jmlr.org/papers/v15/srivastava14a.html)
|2014|SC |     |[Keras](https://github.com/dwright04/Noisy-Labels-with-Bootstrapping)|[Training Deep Neural Networks on Noisy Labels with Bootstrapping](https://arxiv.org/abs/1412.6596)
|2014|NC |     ||[Learning from Noisy Labels with Deep Neural Networks](https://arxiv.org/abs/1406.2080)
|2014|LQA|     ||[Learning from multiple annotators with varying expertise](https://link.springer.com/article/10.1007/s10994-013-5412-1)
|2013|EM |     ||[Boosting in the presence of label noise](https://arxiv.org/abs/1309.6818)
|2013|RL |NIPS ||[Learning with Noisy Labels](https://papers.nips.cc/paper/5073-learning-with-noisy-labels)
|2013|RL |IEEE ||[Noise tolerance under risk minimization](https://arxiv.org/pdf/1109.5231)
|2012|EM |     ||[A noise-detection based AdaBoost algorithm for mislabeled data](https://www.sciencedirect.com/science/article/pii/S0031320312002130)
|2012|RL |ICML ||[Learning to Label Aerial Images from Noisy Data](https://dl.acm.org/citation.cfm?id=3042603)
|2011|EM |     ||[An empirical comparison of two boosting algorithms on real data sets with artificial class noise](https://link.springer.com/chapter/10.1007/978-3-642-22418-8_4)
|2009|LQA|     ||[Supervised learning from multiple experts: whom to trust when everyone lies a bit](https://dl.acm.org/citation.cfm?id=1553488)
|2008|LQA|NIPS ||[Whose vote should count more: Optimal integration of labels from labelers of unknown expertise](https://papers.nips.cc/paper/3644-whose-vote-should-count-more-optimal-integration-of-labels-from-labelers-of-unknown-expertise)
|2006|RL |JASA ||[Convexity, classification, and risk bounds](http://statistics.berkeley.edu/sites/default/files/tech-reports/638.pdf)
|2000|EM |     ||[An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization](https://link.springer.com/article/10.1023/A:1007607513941)

In order to test label-noise-robust algorithms with benchmark datasets (mnist,mnist-fashion,cifar10,cifar100) synthetic noise generation is a necessary step. Following work provides a feature-dependent synthetic noise generation algorithm and pre-generated synthetic noisy labels for mentioned datasets.
* [Label Noise Types and Their Effects on Deep Learning](https://arxiv.org/abs/2003.10471) - [Code](https://github.com/gorkemalgan/corrupting_labels_with_distillation)

List of papers that shed light to label noise phenomenon for deep learning:

|Title                                                                                                                  | Year |
|-----                                                                                                                  | ---- |
|[Image Classification with Deep Learning in the Presence of Noisy Labels: A Survey](https://arxiv.org/abs/1912.05170)  | 2020 |
|[Investigating CNNs' Learning Representation Under Label Noise](https://openreview.net/pdf?id=H1xmqiAqFm)              | 2019 |
|[How Do Neural Networks Overcome Label Noise?](https://openreview.net/forum?id=ryu4RYJPM)                              | 2018 |
|[Deep Learning is Robust to Massive Label Noise](https://arxiv.org/abs/1705.10694)                                     | 2018 |
|[A closer look at memorization in deep networks](https://arxiv.org/abs/1706.05394)                                     | 2017 |
|[Deep Nets Don't Learn via Memorization](https://openreview.net/forum?id=rJv6ZgHYg)                                    | 2017 |
|[On the robustness of convnets to training on noisy labels](https://www.semanticscholar.org/paper/On-the-Robustness-of-ConvNets-to-Training-on-Noisy-Stanford/56c060905d3be9b7ea877e58c36d6856ee1205dd)  | 2017 |
|[A study of the effect of different types of noise on the precision of supervised learning techniques](https://link.springer.com/article/10.1007/s10462-010-9156-z) | 2017 |
|[Understanding deep learning requires rethinking generalization](https://arxiv.org/abs/1611.03530)         | 2016 |
|[A comprehensive introduction to label noise](https://www.elen.ucl.ac.be/Proceedings/esann/esannpdf/es2014-10.pdf)| 2014 |
|[Classification in the Presence of Label Noise: a Survey](https://ieeexplore.ieee.org/document/6685834)    | 2014 |
|[Class noise and supervised learning in medical domains: The effect of feature extraction](https://ieeexplore.ieee.org/abstract/document/1647654/) | 2006 |
|[Class noise vs. attribute noise: A quantitative study](https://link.springer.com/article/10.1007/s10462-004-0751-8) | 2004 |

List of works under label noise beside classification

|Title                                                                                                          |Year|
|-----                                                                                                          |----|
|[Devil is in the Edges: Learning Semantic Boundaries from Noisy Annotations](https://arxiv.org/abs/1904.07934) |2019|
|[Improving Semantic Segmentation via Video Propagation and Label Relaxation](https://arxiv.org/abs/1812.01593) |2018|
|[Learning from weak and noisy labels for semantic segmentation](https://ieeexplore.ieee.org/document/7450177)  |2016|
|[Robustness of conditional GANs to noisy labels](https://arxiv.org/abs/1811.03205)                             |2018|
|[Label-Noise Robust Generative Adversarial Networks](https://arxiv.org/abs/1811.11165)                         |2018|
|[Label-Noise Robust Domain Adaptation](https://proceedings.icml.cc/static/paper_files/icml/2020/1942-Paper.pdf)|2020|

Sources on web
* [Noisy-Labels-Problem-Collection](https://github.com/GuokaiLiu/Noisy-Labels-Problem-Collection)
* [Learning-with-Label-Noise](https://github.com/subeeshvasu/Awesome-Learning-with-Label-Noise)

Clothing1M is a real-world noisy labeled dataset which is widely used for benchmarking. Below is the test accuracies on this dataset. Note that,clothing1M contains spare 50k clean training data, but most of the methods dont use this data for fair comparison. Therefore, here I only listed methods that do not use extra 50k samples. '?' indicates that given work does not mentipon whether they used 50k clean samples or not.

|Title                                                                                                                          |Accuracy|
|-----                                                                                                                          |--------|
|[MetaLabelNet: Learning to Generate Soft-Labels from Noisy-Labels](https://arxiv.org/abs/2103.10869)                           |78.20|
|[Meta Soft Label Generation for Noisy Labels](https://arxiv.org/abs/2007.05836)                                                |76.02|
|[DivideMix: Learning with Noisy Labels as Semi-supervised Learning](https://arxiv.org/abs/2002.07394)?                         |74.76|
|[Cleannet: Transfer Learning for Scalable Image Classifier Training with Label Noise](https://arxiv.org/abs/1711.07131)        |74.69|
|[Deep Self-Learning From Noisy Labels](https://arxiv.org/abs/1908.02160)                                                       |74.45|
|[Limited Gradient Descent: Learning With Noisy Labels](https://arxiv.org/abs/1811.08117)                                       |74.36|
|[Are Anchor Points Really Indispensable in Label-Noise Learning?](https://arxiv.org/abs/1906.00189)                            |74.18|
|[NoiseRank: Unsupervised Label Noise Reduction with Dependence Models](https://arxiv.org/abs/2003.06729)                       |73.77|
|[Learning Adaptive Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2002.06482)                               |73.76|
|[Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting](https://arxiv.org/abs/1902.07379)                         |73.72|
|[Probabilistic End-to-end Noise Correction for Learning with Noisy Labels](https://arxiv.org/abs/1903.07788)                   |73.49|
|[Learning to Learn from Noisy Labeled Data](https://arxiv.org/abs/1812.05214)                                                  |73.47|
|[Improved Mean Absolute Error for Learning Meaningful Patterns from Abnormal Training Data](https://arxiv.org/abs/1903.12141)  |73.20|
|[Safeguarded Dynamic Label Regression for Noisy Supervision](https://arxiv.org/abs/1903.02152)                                 |73.07|
|[Temporal Calibrated Regularization for Robust Noisy Label Learning](https://arxiv.org/abs/2007.00240)                         |72.54|
|[MetaCleaner: Learning to Hallucinate Clean Representations for Noisy-Labeled Visual Recognition](http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_MetaCleaner_Learning_to_Hallucinate_Clean_Representations_for_Noisy-Labeled_Visual_Recognition_CVPR_2019_paper.html)      |72.50|
|[L_DMI : A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise](https://arxiv.org/abs/1909.03388)|72.46|
|[Joint Optimization Framework for Learning with Noisy Labels](https://arxiv.org/abs/1803.11364)                                |72.23|
|[Error-Bounded Correction of Noisy Labels](https://proceedings.icml.cc/static/paper_files/icml/2020/2506-Paper.pdf)            |71.74|
|[Parts-dependent Label Noise: Towards Instance-dependent Label Noise](http://arxiv.org/abs/2006.07836)                         |71.67|
|[Dual T: Reducing Estimation Error for Transition Matrix in Label-noise Learning](http://arxiv.org/abs/2006.07805)?            |71.49|
|[Improving Generalization by Controlling Label-Noise Information in Neural Network Weights](https://arxiv.org/abs/2002.07933)  |71.39|
|[Masking: A new perspective of noisy supervision](https://arxiv.org/abs/1805.08193)                                            |71.10|
|[Symmetric Cross Entropy for Robust Learning with Noisy Labels](https://arxiv.org/abs/1908.06112)                              |71.02|
|[Unsupervised Label Noise Modeling and Loss Correction](https://arxiv.org/abs/1904.11238)                                      |71.00|

Abbreviations for noise types are:
* NC -> Noisy Channel
* LNC -> Label Noise Cleansing
* DP -> Dataset Pruning
* SC -> Sample Choosing
* SIW -> Sample Importance Weighting
* LQA -> Labeler quality assesment
* RL -> Robust Losses
* ML -> Meta Learning
* MIL -> Multiple Instance Learning
* SSL -> Semi Supervised Learning
* R -> Regularizers
* EM -> Ensemble Methods
* O -> Others

Other abbreviations:
* NC -> Neurocomputing
* Tf -> Tensorflow
* Pt -> PyTorch

Starred (*) repos means code is unoffical!